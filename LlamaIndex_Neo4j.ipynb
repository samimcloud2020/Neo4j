{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Neo4jVector\n",
        "**The Neo4j Vector integration supports a number of operations**\n",
        "\n",
        "create vector from langchain documents\n",
        "\n",
        "query vector\n",
        "\n",
        "query vector with additional graph retrieval Cypher query\n",
        "\n",
        "construct vector instance from existing graph data\n",
        "\n",
        "hybrid search\n",
        "\n",
        "metadata filtering"
      ],
      "metadata": {
        "id": "fy1X_J_u-EIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index-llms-openai\n",
        "%pip install llama-index-vector-stores-neo4jvector\n",
        "%pip install llama-index-embeddings-openai\n",
        "%pip install neo4j\n",
        "\n",
        "import os\n",
        "import openai\n",
        "from llama_index.vector_stores.neo4jvector import Neo4jVectorStore\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\"\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "username = \"neo4j\"\n",
        "password = \"pleaseletmein\"\n",
        "url = \"bolt://localhost:7687\"\n",
        "embed_dim = 1536\n",
        "\n",
        "neo4j_vector = Neo4jVectorStore(username, password, url, embed_dim)\n",
        "# load documents\n",
        "documents = SimpleDirectoryReader(\"./data/paul_graham\").load_data()\n",
        "from llama_index.core import StorageContext\n",
        "\n",
        "storage_context = StorageContext.from_defaults(vector_store=neo4j_vector)\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents, storage_context=storage_context\n",
        ")\n",
        "\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What happened at interleaf?\")"
      ],
      "metadata": {
        "id": "Sns8ZSrn-CG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hybrid search\n",
        "Hybrid search combines vector search with **fulltext search with re-ranking and de-duplication of the results.**"
      ],
      "metadata": {
        "id": "w2iiiol6-Rj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neo4j_vector_hybrid = Neo4jVectorStore(\n",
        "    username, password, url, embed_dim, hybrid_search=True\n",
        ")\n",
        "\n",
        "storage_context = StorageContext.from_defaults(\n",
        "    vector_store=neo4j_vector_hybrid\n",
        ")\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents, storage_context=storage_context\n",
        ")\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What happened at interleaf?\")"
      ],
      "metadata": {
        "id": "DOTOOjQB-CKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Metadata filtering\n",
        "Metadata filtering enhances vector search by **allowing searches to be refined based on specific node properties.** This integrated approach ensures more precise and relevant search results by leveraging **both** the **vector similarities and the contextual attributes of the nodes.**"
      ],
      "metadata": {
        "id": "fQX_dI-h-dUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.vector_stores import (\n",
        "    MetadataFilter,\n",
        "    MetadataFilters,\n",
        "    FilterOperator,\n",
        ")\n",
        "\n",
        "filters = MetadataFilters(\n",
        "    filters=[\n",
        "        MetadataFilter(\n",
        "            key=\"theme\", operator=FilterOperator.EQ, value=\"Fiction\"\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "retriever = index.as_retriever(filters=filters)\n",
        "retriever.retrieve(\"What is inception about?\")"
      ],
      "metadata": {
        "id": "cunemVoc-CM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Neo4jPropertyGraphStore\n",
        "The Neo4j Property Graph Store integration is a wrapper for the Neo4j Python driver. It allows querying and updating the Neo4j database in a simplified manner from LlamaIndex. Many integrations allow you to use the Neo4j Property Graph Store as a source of data for LlamaIndex.\n",
        "\n",
        "##Property graph index\n",
        "Knowledge graph index can be used to extract graph representation of information from text and use it to construct a knowledge graph. The graph information can then be retrieved in a RAG application for more accurate responses."
      ],
      "metadata": {
        "id": "X8QjtzpdWqTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index llama-index-graph-stores-neo4j\n",
        "\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core import PropertyGraphIndex\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core.indices.property_graph import SchemaLLMPathExtractor\n",
        "\n",
        "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
        "\n",
        "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
        "graph_store = Neo4jPropertyGraphStore(\n",
        "    username=\"neo4j\",\n",
        "    password=\"password\",\n",
        "    url=\"bolt://localhost:7687\",\n",
        ")\n",
        "# Extract graph from documents\n",
        "index = PropertyGraphIndex.from_documents(\n",
        "    documents,\n",
        "    embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),\n",
        "    kg_extractors=[\n",
        "        SchemaLLMPathExtractor(\n",
        "            llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
        "        )\n",
        "    ],\n",
        "    property_graph_store=graph_store,\n",
        "    show_progress=True,\n",
        ")\n",
        "\n",
        "# Define retriever\n",
        "retriever = index.as_retriever(\n",
        "    include_text=False,  # include source text in returned nodes, default True\n",
        ")\n",
        "results = retriever.retrieve(\"What happened at Interleaf and Viaweb?\")\n",
        "for record in results:\n",
        "    print(record.text)\n",
        "\n",
        "# Question answering\n",
        "query_engine = index.as_query_engine(include_text=True)\n",
        "response = query_engine.query(\"What happened at Interleaf and Viaweb?\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "id": "YE-_lOUw-CPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Property Graph constructing modules\n",
        "LlamaIndex features multiple graph construction modules. Property graph construction in LlamaIndex works by performing a series of kg_extractors on each text chunk, and attaching entities and relations as metadata to each llama-index node."
      ],
      "metadata": {
        "id": "aUo6hH3kWzDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index llama-index-graph-stores-neo4j\n",
        "\n",
        "from typing import Literal\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core import PropertyGraphIndex\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core.indices.property_graph import SchemaLLMPathExtractor\n",
        "\n",
        "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
        "\n",
        "# best practice to use upper-case\n",
        "entities = Literal[\"PERSON\", \"PLACE\", \"ORGANIZATION\"]\n",
        "relations = Literal[\"HAS\", \"PART_OF\", \"WORKED_ON\", \"WORKED_WITH\", \"WORKED_AT\"]\n",
        "\n",
        "# define which entities can have which relations\n",
        "validation_schema = {\n",
        "    \"PERSON\": [\"HAS\", \"PART_OF\", \"WORKED_ON\", \"WORKED_WITH\", \"WORKED_AT\"],\n",
        "    \"PLACE\": [\"HAS\", \"PART_OF\", \"WORKED_AT\"],\n",
        "    \"ORGANIZATION\": [\"HAS\", \"PART_OF\", \"WORKED_WITH\"],\n",
        "}\n",
        "\n",
        "kg_extractor = SchemaLLMPathExtractor(\n",
        "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.0),\n",
        "    possible_entities=entities,\n",
        "    possible_relations=relations,\n",
        "    kg_validation_schema=validation_schema,\n",
        "    # if false, allows for values outside of the schema\n",
        "    # useful for using the schema as a suggestion\n",
        "    strict=True,\n",
        ")\n",
        "graph_store = Neo4jPropertyGraphStore(\n",
        "    username=\"neo4j\",\n",
        "    password=\"password\",\n",
        "    url=\"bolt://localhost:7687\",\n",
        ")\n",
        "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
        "index = PropertyGraphIndex.from_documents(\n",
        "    documents,\n",
        "    kg_extractors=[kg_extractor],\n",
        "    embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),\n",
        "    property_graph_store=graph_store,\n",
        "    show_progress=True,\n",
        ")"
      ],
      "metadata": {
        "id": "7RPg1PF8-CR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Property graph querying modules\n",
        "Labeled property graphs can be queried in several ways to retrieve nodes and paths. And in LlamaIndex, you can combine several node retrieval methods at once!"
      ],
      "metadata": {
        "id": "zccmSSquX6Yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.retrievers import (\n",
        "    CustomPGRetriever,\n",
        "    VectorContextRetriever,\n",
        "    TextToCypherRetriever,\n",
        ")\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.graph_stores import PropertyGraphStore\n",
        "from llama_index.core.vector_stores.types import VectorStore\n",
        "from llama_index.core.embeddings import BaseEmbedding\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "from llama_index.core.llms import LLM\n",
        "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
        "\n",
        "\n",
        "from typing import Optional, Any, Union\n",
        "\n",
        "\n",
        "class MyCustomRetriever(CustomPGRetriever):\n",
        "    \"\"\"Custom retriever with cohere reranking.\"\"\"\n",
        "\n",
        "    def init(\n",
        "        self,\n",
        "        ## vector context retriever params\n",
        "        embed_model: Optional[BaseEmbedding] = None,\n",
        "        vector_store: Optional[VectorStore] = None,\n",
        "        similarity_top_k: int = 4,\n",
        "        path_depth: int = 1,\n",
        "        ## text-to-cypher params\n",
        "        llm: Optional[LLM] = None,\n",
        "        text_to_cypher_template: Optional[Union[PromptTemplate, str]] = None,\n",
        "        ## cohere reranker params\n",
        "        cohere_api_key: Optional[str] = None,\n",
        "        cohere_top_n: int = 2,\n",
        "        **kwargs: Any,\n",
        "    ) -> None:\n",
        "        \"\"\"Uses any kwargs passed in from class constructor.\"\"\"\n",
        "\n",
        "        self.vector_retriever = VectorContextRetriever(\n",
        "            self.graph_store,\n",
        "            include_text=self.include_text,\n",
        "            embed_model=embed_model,\n",
        "            vector_store=vector_store,\n",
        "            similarity_top_k=similarity_top_k,\n",
        "            path_depth=path_depth,\n",
        "        )\n",
        "\n",
        "        self.cypher_retriever = TextToCypherRetriever(\n",
        "            self.graph_store,\n",
        "            llm=llm,\n",
        "            text_to_cypher_template=text_to_cypher_template\n",
        "            ## NOTE: you can attach other parameters here if you'd like\n",
        "        )\n",
        "\n",
        "        self.reranker = CohereRerank(\n",
        "            api_key=cohere_api_key, top_n=cohere_top_n\n",
        "        )\n",
        "\n",
        "    def custom_retrieve(self, query_str: str) -> str:\n",
        "        \"\"\"Define custom retriever with reranking.\n",
        "\n",
        "        Could return `str`, `TextNode`, `NodeWithScore`, or a list of those.\n",
        "        \"\"\"\n",
        "        nodes_1 = self.vector_retriever.retrieve(query_str)\n",
        "        nodes_2 = self.cypher_retriever.retrieve(query_str)\n",
        "        reranked_nodes = self.reranker.postprocess_nodes(\n",
        "            nodes_1 + nodes_2, query_str=query_str\n",
        "        )\n",
        "\n",
        "        ## TMP: please change\n",
        "        final_text = \"\\n\\n\".join(\n",
        "            [n.get_content(metadata_mode=\"llm\") for n in reranked_nodes]\n",
        "        )\n",
        "\n",
        "        return final_text\n",
        "\n",
        "custom_sub_retriever = MyCustomRetriever(\n",
        "    index.property_graph_store,\n",
        "    include_text=True,\n",
        "    vector_store=index.vector_store,\n",
        "    cohere_api_key=\"...\",\n",
        ")\n",
        "\n",
        "query_engine = RetrieverQueryEngine.from_args(\n",
        "    index.as_retriever(sub_retrievers=[custom_sub_retriever]), llm=llm\n",
        ")\n",
        "\n",
        "response = query_engine.query(\"Did the author like programming?\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "id": "e1j5O3YxX9Vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nEv1_L97YAVY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}